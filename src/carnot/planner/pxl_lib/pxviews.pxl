R"(
# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0


import px


def pod_resource_stats(start_time, end_time):
    ''' Resource usage statistics for each pod.
    Provides start_time, number of containers, associated node, cpu, disk I/O and memory info.
    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    '''
    pod_metadata_df = px.DataFrame(table='process_stats', start_time=start_time, end_time=end_time)
    pod_metadata_df.pod = pod_metadata_df.ctx['pod_name']
    pod_metadata_df.container = pod_metadata_df.ctx['container_id']
    pod_metadata_df = pod_metadata_df.groupby(['pod', 'container']).agg()
    pod_metadata_df = pod_metadata_df.groupby(['pod']).agg(container_count=('container', px.count))

    df = px.DataFrame(table='process_stats', start_time=start_time, end_time=end_time)
    df['pod_id'] = df.ctx['pod_id']
    df = df.groupby(['pod_id', 'upid']).agg(
        rss=('rss_bytes', px.mean),
        vsize=('vsize_bytes', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ns_max=('cpu_utime_ns', px.max),
        cpu_utime_ns_min=('cpu_utime_ns', px.min),
        cpu_ktime_ns_max=('cpu_ktime_ns', px.max),
        cpu_ktime_ns_min=('cpu_ktime_ns', px.min),
        read_bytes_max=('read_bytes', px.max),
        read_bytes_min=('read_bytes', px.min),
        write_bytes_max=('write_bytes', px.max),
        write_bytes_min=('write_bytes', px.min),
        rchar_bytes_max=('rchar_bytes', px.max),
        rchar_bytes_min=('rchar_bytes', px.min),
        wchar_bytes_max=('wchar_bytes', px.max),
        wchar_bytes_min=('wchar_bytes', px.min),
        time_max=('time_', px.max),
        time_min=('time_', px.min),
    )

    df.timestamp = df.time_max
    # Convert counters into gauges by subtracting the value at the start of the window from the end of the window.
    df.cpu_utime_ns = df.cpu_utime_ns_max - df.cpu_utime_ns_min
    df.cpu_ktime_ns = df.cpu_ktime_ns_max - df.cpu_ktime_ns_min
    df.read_bytes = df.read_bytes_max - df.read_bytes_min
    df.write_bytes = df.write_bytes_max - df.write_bytes_min
    df.rchar_bytes = df.rchar_bytes_max - df.rchar_bytes_min
    df.wchar_bytes = df.wchar_bytes_max - df.wchar_bytes_min
    df.window = df.time_max - df.time_min

    # Sum up the UPID values.
    df = df.groupby(['pod_id']).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.sum),
        cpu_utime_ns=('cpu_utime_ns', px.sum),
        read_bytes=('read_bytes', px.sum),
        write_bytes=('write_bytes', px.sum),
        rchar_bytes=('rchar_bytes', px.sum),
        wchar_bytes=('wchar_bytes', px.sum),
        rss=('rss', px.sum),
        vsize=('vsize', px.sum),
        # We take the max across all windows for a pod as a best effort to
        # show the rate of resource usage for the pod. That means dead pods that
        # used a large amount of resources during their lifetimes will appear
        # as if they are still using the large resources if the window includes
        # their data.
        window=('window', px.max),
        timestamp=('timestamp', px.max),
    )

    # If the window size is 0, we set window to 1 to avoid NaNs.
    # the rates will be calculated to 0 because a zero window size
    # will mean that min == max above for all counters.
    df.window = px.select(df.window > 0, df.window, 1)
    # Divide the values by the size of the window to get the rate.
    df.actual_disk_read_throughput = df.read_bytes / df.window
    df.actual_disk_write_throughput = df.write_bytes / df.window
    df.total_disk_read_throughput = df.rchar_bytes / df.window
    df.total_disk_write_throughput = df.wchar_bytes / df.window

    # Sum the cpu_usage into one value.
    df.cpu_usage = px.Percent((df.cpu_ktime_ns + df.cpu_utime_ns) / df.window)
    df.pod = df.ctx['pod']
    df = df.drop(['window', 'cpu_ktime_ns', 'cpu_utime_ns', 'read_bytes', 'rchar_bytes', 'write_bytes', 'wchar_bytes'])
    df = df.merge(pod_metadata_df, how='inner', left_on='pod', right_on='pod',
                  suffixes=['', '_x']).drop(['pod_x'])

    df.service = df.ctx['service_name']
    df.node = df.ctx['node_name']
    df.namespace = df.ctx['namespace']
    df.start_time = px.pod_name_to_start_time(df.pod)
    df.status = px.pod_name_to_status(df.pod)
    return df[['pod_id', 'pod', 'service', 'node', 'namespace', 'container_count',
               'start_time', 'status', 'cpu_usage', 'rss', 'vsize',
               'actual_disk_read_throughput', 'actual_disk_write_throughput',
               'total_disk_read_throughput', 'total_disk_write_throughput']]


def _http_events(start_time, end_time, include_health_checks, include_ready_checks, include_unresolved_inbound):
    ''' Returns a dataframe of http events with health/ready requests filtered out and a
        failure field added.

    Note this script is prefixed with an underscore and therefore at risk of changing in the future.
    Rely on this function at your own risk.

    Args:
    @start_time: The timestamp of data to start at.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time, end_time=end_time)
    df.failure = df.resp_status >= 400

    filter_out_conds = ((df.req_path != '/healthz' or include_health_checks) and (
        df.req_path != '/readyz' or include_ready_checks)) and (
        df['remote_addr'] != '-' or include_unresolved_inbound)
    df = df[filter_out_conds]

    return df


def inbound_http_summary(start_time, end_time):
    ''' Gets a summary of requests inbound to `pod`.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    '''
    df = _http_events(start_time,
                     end_time,
                     include_health_checks=False,
                     include_ready_checks=False,
                     include_unresolved_inbound=False)
    df.pod_id = df.ctx['pod_id']

    # Filter only to inbound pod traffic (server-side).
    # Don't include traffic initiated by this pod to an external location.
    df = df[df.trace_role == 2]

    df = df.groupby(['pod_id', 'remote_addr']).agg(
        latency=('latency', px.quantiles),
        num_requests=('time_', px.count),
        stop_time=('time_', px.max),
        num_errors=('failure', px.sum),

        # TODO(philkuz) calculate this from the conn_stats table.
        inbound_bytes=('req_body_size', px.sum),
        outbound_bytes=('resp_body_size', px.sum)
    )

    df.requesting_ip = df.remote_addr
    df.requesting_pod_id = px.ip_to_pod_id(df.remote_addr)
    df.requesting_pod = px.pod_id_to_pod_name(df.requesting_pod_id)
    df.requesting_svc = px.pod_id_to_service_name(df.requesting_pod_id)
    df.time_ = df.stop_time

    return df[['time_', 'pod_id', 'requesting_ip', 'requesting_pod', 'requesting_svc', 'latency',
                         'num_errors', 'num_requests', 'inbound_bytes', 'outbound_bytes']]


def http_graph(start_time, end_time):
    ''' Calculates the graph of all http requests made within a cluster.

    Calculates a graph of all the pods that make http requests to each other. Users
    can filter the graph per namespace, service, pod, or ip.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    '''
    # TODO(philkuz) fix pxviews can't use False because it's not in scope.
    df = _http_events(start_time,
                     end_time,
                     include_health_checks=False,
                     include_ready_checks=False,
                     include_unresolved_inbound=False)
    df.pod_id = df.ctx['pod_id']
    df = df.groupby(['pod_id', 'remote_addr', 'trace_role']).agg(
        latency=('latency', px.quantiles),
        total_request_count=('latency', px.count)
        num_requests=('time_', px.count),
        stop_time=('time_', px.max),
        num_errors=('failure', px.sum),
        inbound_bytes=('req_body_size', px.sum),
        outbound_bytes=('resp_body_size', px.sum)
    )

    df.traced_pod = df.ctx['pod']
    df.traced_ip = px.pod_name_to_pod_ip(df.traced_pod)
    df.traced_service = df.ctx['service']

    # Get the traced and remote pod/service/IP information.
    df.remote_ip = df.remote_addr
    localhost_ip_regexp = r'127\.0\.0\.[0-9]+'
    df.is_remote_addr_localhost = px.regex_match(localhost_ip_regexp, df.remote_ip)
    df.remote_pod = px.select(df.is_remote_addr_localhost,
                              'localhost:' + df.traced_pod,
                              px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_ip)))
    df.remote_service = px.select(df.is_remote_addr_localhost,
                                  'localhost:' + df.traced_service,
                                  px.service_id_to_service_name(px.ip_to_service_id(df.remote_ip)))

    # Assign requestor and responder based on the trace_role.
    df.is_server_side_tracing = df.trace_role == 2
    df.responder_ip = px.select(df.is_server_side_tracing, df.traced_ip, df.remote_ip)
    df.requestor_ip = px.select(df.is_server_side_tracing, df.remote_ip, df.traced_ip)

    df.responder_pod = px.select(df.is_server_side_tracing, df.traced_pod, df.remote_pod)
    df.requestor_pod = px.select(df.is_server_side_tracing, df.remote_pod, df.traced_pod)

    df.responder_service = px.select(df.is_server_side_tracing, df.traced_service, df.remote_service)
    df.requestor_service = px.select(df.is_server_side_tracing, df.remote_service, df.traced_service)

    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency, 'p99')))

    df = df.groupby([
        'responder_pod',
        'requestor_pod',
        'responder_ip',
        'requestor_ip',
        'responder_service',
        'requestor_service'
    ]).agg(
        # TODO(philkuz) build a combine_quantiles udf that does this properly.
        # latency=('latency', px.combine_quantiles),
        latency_p50=('latency_p50', px.mean),
        latency_p90=('latency_p90', px.mean),
        latency_p99=('latency_p99', px.mean),
        num_requests=('num_requests', px.sum),
        num_errors=('num_errors', px.sum),
        inbound_bytes=('inbound_bytes', px.sum),
        outbound_bytes=('outbound_bytes', px.sum),
        stop_time=('stop_time', px.max),
    )

    df.time_ = df.stop_time
    return df[['time_', 'responder_pod', 'requestor_pod',
               'responder_ip', 'requestor_ip','responder_service', 'requestor_service',
               'latency_p50', 'latency_p90', 'latency_p99', 'num_requests',
               'num_errors', 'inbound_bytes', 'outbound_bytes']]


def container_process_timeseries(start_time, end_time, window_ns):
    ''' Compute the timeseries of CPU, memory, and IO usage for each container
    sourced from the `process_stats` table.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    @window_ns The window size in nanoseconds for the timeseries.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time, end_time=end_time)
    df.timestamp = px.bin(df.time_, window_ns)
    df.container = df.ctx['container_name']
    df.pod_id = df.ctx['pod_id']

    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby(['upid', 'container','pod_id', 'timestamp']).agg(
        rss=('rss_bytes', px.mean),
        vsize=('vsize_bytes', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ns_max=('cpu_utime_ns', px.max),
        cpu_utime_ns_min=('cpu_utime_ns', px.min),
        cpu_ktime_ns_max=('cpu_ktime_ns', px.max),
        cpu_ktime_ns_min=('cpu_ktime_ns', px.min),
        read_bytes_max=('read_bytes', px.max),
        read_bytes_min=('read_bytes', px.min),
        write_bytes_max=('write_bytes', px.max),
        write_bytes_min=('write_bytes', px.min),
        rchar_bytes_max=('rchar_bytes', px.max),
        rchar_bytes_min=('rchar_bytes', px.min),
        wchar_bytes_max=('wchar_bytes', px.max),
        wchar_bytes_min=('wchar_bytes', px.min),
    )

    # Next calculate cpu usage and memory stats per window.
    df.cpu_utime_ns = df.cpu_utime_ns_max - df.cpu_utime_ns_min
    df.cpu_ktime_ns = df.cpu_ktime_ns_max - df.cpu_ktime_ns_min
    df.actual_disk_read_throughput = (df.read_bytes_max - df.read_bytes_min) / window_ns
    df.actual_disk_write_throughput = (df.write_bytes_max - df.write_bytes_min) / window_ns
    df.total_disk_read_throughput = (df.rchar_bytes_max - df.rchar_bytes_min) / window_ns
    df.total_disk_write_throughput = (df.wchar_bytes_max - df.wchar_bytes_min) / window_ns


    # Then aggregate per container.
    df = df.groupby(['timestamp', 'pod_id', 'container']).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.sum),
        cpu_utime_ns=('cpu_utime_ns', px.sum),
        actual_disk_read_throughput=('actual_disk_read_throughput', px.sum),
        actual_disk_write_throughput=('actual_disk_write_throughput', px.sum),
        total_disk_read_throughput=('total_disk_read_throughput', px.sum),
        total_disk_write_throughput=('total_disk_write_throughput', px.sum),
        rss=('rss', px.sum),
        vsize=('vsize', px.sum),
    )

    # Finally, calculate total (kernel + user time)  percentage used over window.
    df.cpu_usage = px.Percent((df.cpu_ktime_ns + df.cpu_utime_ns) / window_ns)
    df.time_ = df.timestamp
    return df[['time_', 'pod_id', 'container', 'cpu_usage', 'actual_disk_read_throughput',
            'actual_disk_write_throughput', 'total_disk_read_throughput',
            'total_disk_write_throughput', 'rss', 'vsize']]


def pod_network_timeseries(start_time, end_time, window_ns):
    ''' Compute the timeseries of network events for each pod.

    Returns timeseries data summarizing the bytes, drops, and errors for
    the incoming (rx) and outgoing(tx) network connections for each pod.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    @window_ns The window size in nanoseconds for the timeseries.
    '''
    df = px.DataFrame(table='network_stats', start_time=start_time, end_time=end_time)
    df.timestamp = px.bin(df.time_, window_ns)

    # First calculate network usage by node over all windows.
    # Data is sharded by Pod in network_stats.
    df = df.groupby(['timestamp', 'pod_id']).agg(
        rx_bytes_end=('rx_bytes', px.max),
        rx_bytes_start=('rx_bytes', px.min),
        tx_bytes_end=('tx_bytes', px.max),
        tx_bytes_start=('tx_bytes', px.min),
        tx_errors_end=('tx_errors', px.max),
        tx_errors_start=('tx_errors', px.min),
        rx_errors_end=('rx_errors', px.max),
        rx_errors_start=('rx_errors', px.min),
        tx_drops_end=('tx_drops', px.max),
        tx_drops_start=('tx_drops', px.min),
        rx_drops_end=('rx_drops', px.max),
        rx_drops_start=('rx_drops', px.min),
    )

    # Calculate the network statistics rate over the window.
    # We subtract the counter value at the beginning ('_start')
    # from the value at the end ('_end').
    df.rx_bytes_per_ns = (df.rx_bytes_end - df.rx_bytes_start) / window_ns
    df.tx_bytes_per_ns = (df.tx_bytes_end - df.tx_bytes_start) / window_ns
    df.rx_drops_per_ns = (df.rx_drops_end - df.rx_drops_start) / window_ns
    df.tx_drops_per_ns = (df.tx_drops_end - df.tx_drops_start) / window_ns
    df.rx_errors_per_ns = (df.rx_errors_end - df.rx_errors_start) / window_ns
    df.tx_errors_per_ns = (df.tx_errors_end - df.tx_errors_start) / window_ns

    df.time_ = df.timestamp
    return df[['time_', 'pod_id', 'rx_bytes_per_ns', 'tx_bytes_per_ns',
               'rx_drops_per_ns', 'tx_drops_per_ns', 'rx_errors_per_ns', 'tx_errors_per_ns']]

)"
